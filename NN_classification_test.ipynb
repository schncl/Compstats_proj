{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, fixed, widgets\n",
    "from tqdm import tqdm\n",
    "# visulisation function\n",
    "from functions.visualisations import (\n",
    "    histogram_trace, plot_y_timeseries, \n",
    "    plot_ycorr_scatter, boxplot_weights,\n",
    "    plot_linear_data\n",
    ")\n",
    "\n",
    "from types import MethodType\n",
    "\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(self,X):\n",
    "    l1_z=np.dot(X,self.l1_weights)+self.l1_biases\n",
    "    self.l1_output=self.sigmoid(l1_z)\n",
    "    l2_z=np.dot(self.l1_output,self.l2_weights)+self.l2_biases\n",
    "    self.l2_output=self.sigmoid(l2_z)\n",
    "\n",
    "    return self.l2_output\n",
    "\n",
    "\n",
    "def backward_pass(self,X,Y):\n",
    "    l2_delta=(Y-self.l2_output)*(self.l2_output*(1-self.l2_output))\n",
    "    l2_weights_delta=np.outer(self.l1_output,l2_delta)\n",
    "\n",
    "    l1_delta=np.dot(l2_delta,self.l2_weights.T)*(self.l1_output*(1-self.l1_output))\n",
    "    l1_weights_delta=np.outer(X,l1_delta)\n",
    "\n",
    "    self.l2_weights+=self.lrate*l2_weights_delta\n",
    "    self.l2_biases+=self.lrate*l2_delta\n",
    "    self.l1_weights+=self.lrate*l1_weights_delta\n",
    "    self.l1_biases+=self.lrate*l1_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers_sizes,learning_rate=0.01):\n",
    "        self.input_num=layers_sizes[0]\n",
    "        self.hidden_num=layers_sizes[1]\n",
    "        self.output_num=layers_sizes[2]\n",
    "\n",
    "        self.n_params=(self.input_num*self.hidden_num)*(self.hidden_num*self.output_num)+\\\n",
    "            self.hidden_num+self.output_num\n",
    "\n",
    "        self.lrate=learning_rate\n",
    "\n",
    "        self.initialise_network()\n",
    "\n",
    "        self.forward_pass=MethodType(forward_pass,self)\n",
    "        self.backward_pass=MethodType(backward_pass,self)\n",
    "    \n",
    "    def initialise_network(self):\n",
    "\n",
    "        self.l1_weights=np.random.normal(\n",
    "            loc=0,scale=1/np.sqrt(self.input_num),\n",
    "            size=(self.input_num,self.hidden_num)\n",
    "        )\n",
    "        self.l1_biases=np.random.normal(\n",
    "            loc=0,scale=1/np.sqrt(self.hidden_num),\n",
    "            size=((self.hidden_num,))\n",
    "        )\n",
    "        self.l1_output=np.zeros((1,self.hidden_num))\n",
    "\n",
    "        self.l2_weights=np.random.normal(\n",
    "            loc=0,scale=1/np.sqrt(self.hidden_num),\n",
    "            size=(self.hidden_num,self.output_num)\n",
    "        )\n",
    "        self.l2_biases=np.random.normal(\n",
    "            loc=0,scale=1/np.sqrt(self.hidden_num),\n",
    "            size=(self.output_num,)\n",
    "        )\n",
    "        self.l2_output=np.zeros((1,self.output_num))\n",
    "\n",
    "    def evaluate_proposal(self,x_data,theta):\n",
    "        self.decode(theta)\n",
    "        size=x_data.shape[0]\n",
    "        fx=np.zeros(size)\n",
    "        #prob=np.zeros((size,self.output_num))\n",
    "\n",
    "        for i in range(0,size):\n",
    "            fx_tmp=self.forward_pass(x_data[i,])\n",
    "            fx[i]=fx_tmp\n",
    "        \n",
    "        return fx\n",
    "\n",
    "\n",
    "    def langevin_gradient(self,x_data,y_data,theta,depth):\n",
    "        self.decode(theta)\n",
    "        size=x_data.shape[0]\n",
    "\n",
    "        for _ in range(0,depth):\n",
    "            for ii in range(0,size):\n",
    "                self.forward_pass(x_data[ii,])\n",
    "                self.backward_pass(x_data[ii,],y_data[ii])\n",
    "        theta_updated=self.encode()\n",
    "        return theta_updated\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def encode(self):\n",
    "        w1=self.l1_weights.ravel()\n",
    "        w2=self.l2_weights.ravel()\n",
    "        theta=np.concatenate([w1,w2,self.l1_biases,self.l2_biases])\n",
    "        return theta\n",
    "\n",
    "    def decode(self,theta):\n",
    "        w_layer1size=self.input_num*self.hidden_num\n",
    "        w_layer2size=self.hidden_num*self.output_num\n",
    "        w_layer1=theta[0:w_layer1size]\n",
    "        self.l1_weights=np.reshape(w_layer1,(self.input_num,self.hidden_num))\n",
    "        w_layer2=theta[w_layer1size:w_layer1size+w_layer2size]\n",
    "        self.l2_weights=np.reshape(w_layer2,(self.hidden_num,self.output_num))\n",
    "\n",
    "        self.l1_biases=theta[w_layer1size+w_layer2size:w_layer1size+w_layer2size+self.hidden_num]\n",
    "        self.l2_biases=theta[w_layer1size+w_layer2size+self.hidden_num:w_layer1size + w_layer2size + self.hidden_num + self.output_num]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_function(self,theta,tausq,test=False):\n",
    "    if test:\n",
    "        x_data=self.x_test\n",
    "        y_data=self.y_test\n",
    "    else:\n",
    "        x_data=self.x_data\n",
    "        y_data=self.y_data\n",
    "    \n",
    "    model_prediction=self.model.evaluate_proposal(x_data,theta)\n",
    "    model_simulation=model_prediction+np.random.normal(0,tausq,size=model_prediction.shape)\n",
    "    accuracy=self.rmse(model_prediction,y_data)\n",
    "    log_likelihood=np.sum(-0.5*np.log(2*np.pi*tausq)-0.5*np.square(y_data-model_prediction)/tausq)\n",
    "    return [log_likelihood, model_prediction, model_simulation, accuracy]\n",
    "\n",
    "def prior(self, sigma_squared, nu_1, nu_2, theta, tausq): \n",
    "\n",
    "    n_params = self.theta_size # number of parameters in model\n",
    "    part1 = -1 * (n_params / 2) * np.log(sigma_squared)\n",
    "    part2 = 1 / (2 * sigma_squared) * (sum(np.square(theta)))\n",
    "    log_prior = part1 - part2 - (1 + nu_1) * np.log(tausq) - (nu_2 / tausq)\n",
    "    return log_prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(self):\n",
    "    pos_theta=np.ones((self.n_samples,self.theta_size))\n",
    "    pos_tau=np.ones((self.n_samples,1))\n",
    "    pos_eta=np.ones((self.n_samples,1))\n",
    "\n",
    "    pred_y=np.zeros((self.n_samples,self.x_data.shape[0]))\n",
    "    sim_y=np.zeros((self.n_samples,self.x_data.shape[0]))\n",
    "    rmse_data=np.zeros(self.n_samples)\n",
    "\n",
    "    test_pred_y=np.ones((self.n_samples,self.x_test.shape[0]))\n",
    "    test_sim_y=np.ones((self.n_samples,self.x_test.shape[0]))\n",
    "    test_rmse_data=np.zeros(self.n_samples)\n",
    "\n",
    "    theta=np.random.randn(self.theta_size)\n",
    "\n",
    "    pred_y[0,]=self.model.evaluate_proposal(self.x_data,theta)\n",
    "\n",
    "    eta=np.log(np.var(pred_y[0,]-self.y_data))\n",
    "    tau_proposal=np.exp(eta)\n",
    "\n",
    "    sigma_squared=self.sigma_squared\n",
    "    nu_1=self.nu_1\n",
    "    nu_2=self.nu_2\n",
    "\n",
    "    prior_val=self.prior(sigma_squared,nu_1,nu_2,theta,tau_proposal)\n",
    "    [likelihood, pred_y[0,], sim_y[0,], rmse_data[0]] = self.likelihood_function(theta, tau_proposal)\n",
    "\n",
    "    n_accept=0\n",
    "    n_langevin=0\n",
    "\n",
    "    for ii in tqdm(np.arange(1,self.n_samples)):\n",
    "        theta_proposal=theta+np.random.normal(0,self.step_theta,self.theta_size)\n",
    "        lx=np.random.uniform(0,1,1)\n",
    "        if(self.use_langevin_gradients is True) and (lx<self.l_prob):\n",
    "            theta_gd=self.model.langevin_gradient(self.x_data,self.y_data,theta.copy(),self.sgd_depth)\n",
    "            theta_proposal=np.random.normal(theta_gd,self.step_theta,self.theta_size)\n",
    "            theta_proposal_gd=self.model.langevin_gradient(self.x_data,self.y_data,theta_proposal.copy(),self.sgd_depth)\n",
    "\n",
    "            wc_delta=(theta-theta_proposal_gd)\n",
    "            wp_delta=(theta_proposal-theta_gd)\n",
    "\n",
    "            sigma_sq=self.step_theta\n",
    "            first=-0.5*np.sum(wc_delta*wc_delta)/sigma_sq\n",
    "            second=-0.5*np.sum(wp_delta*wp_delta)/sigma_sq\n",
    "            diff_prop=first-second\n",
    "            n_langevin+=1\n",
    "        else:\n",
    "            diff_prop=0\n",
    "            theta_proposal=np.random.normal(theta,self.step_theta)\n",
    "\n",
    "    eta_proposal=eta+np.random.normal(0,self.step_eta,1)\n",
    "    tau_proposal=np.exp(eta_proposal)\n",
    "\n",
    "    prior_proposal=self.prior(sigma_squared,nu_1,nu_2,theta_proposal,tau_proposal)\n",
    "    [likelihood_proposal, pred_y[ii,], sim_y[ii,], rmse_data[ii]] = self.likelihood_function(\n",
    "        theta_proposal, tau_proposal\n",
    "    )\n",
    "\n",
    "    [_, test_pred_y[ii,], test_sim_y[ii,], test_rmse_data[ii]] = self.likelihood_function(\n",
    "            theta_proposal, tau_proposal, test=True\n",
    "        )\n",
    "    diff_likelihood=likelihood_proposal-likelihood\n",
    "    diff_prior=prior_proposal-prior_val\n",
    "    mh_prob=min(1,np.exp(diff_likelihood+diff_prior+diff_prop))\n",
    "    u=np.random.uniform(0,1)\n",
    "\n",
    "    if u < mh_prob:\n",
    "        n_accept+=1\n",
    "        likelihood=likelihood_proposal\n",
    "        prior_val=prior_proposal\n",
    "        theta=theta_proposal\n",
    "        eta=eta_proposal\n",
    "        pos_theta[ii,]=theta_proposal\n",
    "        pos_tau[ii,]=tau_proposal\n",
    "        pos_eta[ii,]=eta_proposal\n",
    "    else:\n",
    "        pos_theta[ii,]=pos_theta[ii-1,]\n",
    "        pos_tau[ii,]= pos_tau[ii-1,]\n",
    "        pos_eta[ii,]= pos_eta[ii-1,]\n",
    "\n",
    "    accept_ratio=(n_accept/self.n_samples)*100\n",
    "    print('{:.3}% were acepted'.format(accept_ratio))\n",
    "    \n",
    "    self.pos_theta = pos_theta[self.n_burnin:, ]\n",
    "    \n",
    "    self.pos_tau = pos_tau[self.n_burnin:, ]\n",
    "    \n",
    "    self.pos_eta = pos_eta[self.n_burnin:, ]\n",
    "    \n",
    "\n",
    "    results_dict = {'w{}'.format(_): self.pos_theta[:, _].squeeze() for _ in range(self.theta_size-2)}\n",
    "    results_dict['b0'] = self.pos_theta[:, self.theta_size-2].squeeze()\n",
    "    results_dict['b1'] = self.pos_theta[:, self.theta_size-1].squeeze()    \n",
    "    results_dict['tau'] = self.pos_tau.squeeze()\n",
    " \n",
    "    # return the predictions\n",
    "    pred_dict = {}\n",
    "    pred_dict['train_pred'] = pred_y[self.n_burnin:,:]\n",
    "    pred_dict['train_sim'] = sim_y[self.n_burnin:,:]\n",
    "    pred_dict['test_pred'] = test_pred_y[self.n_burnin:,:]\n",
    "    pred_dict['test_sim'] = test_sim_y[self.n_burnin:,:]\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(\n",
    "        results_dict\n",
    "    )\n",
    "\n",
    "    return results_df, pred_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC: \n",
    "    def __init__(self,model,n_samples,n_burnin,x_data,y_data,x_test,y_test):\n",
    "        self.n_samples = n_samples # number of MCMC samples\n",
    "        self.n_burnin = n_burnin # number of burn-in samples\n",
    "        self.x_data = x_data # (N x num_features)\n",
    "        self.y_data = y_data # (N x 1)\n",
    "        self.x_test = x_test # (Nt x num_features)\n",
    "        self.y_test = y_test # (Nt x 1)\n",
    "\n",
    "        # MCMC sampler hyperparameters - defines how much variation you need in changes to theta, tau\n",
    "        self.step_theta = 0.025;  \n",
    "        self.step_eta = 0.2; # note eta is used as tau in the sampler to consider log scale.\n",
    "        # Hyperpriors\n",
    "        self.sigma_squared = 25\n",
    "        self.nu_1 = 0\n",
    "        self.nu_2 = 0\n",
    "        self.model = model\n",
    "        self.use_langevin_gradients = True\n",
    "        self.sgd_depth = 1\n",
    "        self.l_prob = 0.5 # likelihood prob\n",
    "        self.theta_size = self.model.n_params # weights for each feature and a bias term\n",
    "\n",
    "        # store output\n",
    "        self.pos_theta = None\n",
    "        self.pos_tau = None\n",
    "        self.pos_eta = None\n",
    "        self.rmse_data = None\n",
    "\n",
    "        # functions defined above - this is poor practice, but done for readability \n",
    "        # and clarity\n",
    "        self.likelihood_function = MethodType(likelihood_function, self)\n",
    "        self.prior = MethodType(prior, self)\n",
    "        self.sampler = MethodType(sampler, self)\n",
    "\n",
    "    def rmse(self, predictions, targets):\n",
    "        '''\n",
    "        Additional error metric - root mean square error\n",
    "        '''\n",
    "        return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (298, 5)\n",
      "Test data shape: (198, 5)\n"
     ]
    }
   ],
   "source": [
    "name=\"Sunspot\"\n",
    "train_data=np.loadtxt(\"data/{}/train.txt\".format(name))\n",
    "test_data=np.loadtxt(\"data/{}/test.txt\".format(name))\n",
    "print('Training data shape: {}'.format(train_data.shape))\n",
    "print('Test data shape: {}'.format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/43g5ft3d50d1_ftbvq8_x9rc0000gn/T/ipykernel_12116/446482416.py:47: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  fx[i]=fx_tmp\n",
      "  0%|          | 0/4999 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (411,) and arg 1 with shape (61,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m nn_model \u001b[39m=\u001b[39m NeuralNetwork(layer_sizes,learning_rate)\n\u001b[1;32m     15\u001b[0m mcmc \u001b[39m=\u001b[39m MCMC(nn_model,n_samples, burn_in, x_data, y_data, x_test, y_test)\n\u001b[0;32m---> 17\u001b[0m result,pred\u001b[39m=\u001b[39mmcmc\u001b[39m.\u001b[39;49msampler()\n",
      "Cell \u001b[0;32mIn[29], line 36\u001b[0m, in \u001b[0;36msampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_langevin_gradients \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m) \u001b[39mand\u001b[39;00m (lx\u001b[39m<\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_prob):\n\u001b[1;32m     35\u001b[0m     theta_gd\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlangevin_gradient(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_data,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_data,theta\u001b[39m.\u001b[39mcopy(),\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msgd_depth)\n\u001b[0;32m---> 36\u001b[0m     theta_proposal\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mnormal(theta_gd,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_theta,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtheta_size)\n\u001b[1;32m     37\u001b[0m     theta_proposal_gd\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlangevin_gradient(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_data,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_data,theta_proposal\u001b[39m.\u001b[39mcopy(),\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msgd_depth)\n\u001b[1;32m     39\u001b[0m     wc_delta\u001b[39m=\u001b[39m(theta\u001b[39m-\u001b[39mtheta_proposal_gd)\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1557\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:600\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:517\u001b[0m, in \u001b[0;36mnumpy.random._common.cont_broadcast_2\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m__init__.cython-30.pxd:780\u001b[0m, in \u001b[0;36mnumpy.PyArray_MultiIterNew3\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape.  Mismatch is between arg 0 with shape (411,) and arg 1 with shape (61,)."
     ]
    }
   ],
   "source": [
    "## MCMC Settings and Setup\n",
    "n_samples       = 5000 # number of samples to draw from the posterior\n",
    "burn_in         = int(n_samples* 0.5) # number of samples to discard before recording draws from the posterior\n",
    "hidden          = 10\n",
    "learning_rate   = 0.01\n",
    "\n",
    "x_data = train_data[:,:-1]\n",
    "y_data = train_data[:,-1]\n",
    "x_test = test_data[:,:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "layer_sizes = [x_data.shape[1], hidden, 1]\n",
    "\n",
    "nn_model = NeuralNetwork(layer_sizes,learning_rate)\n",
    "mcmc = MCMC(nn_model,n_samples, burn_in, x_data, y_data, x_test, y_test)\n",
    "\n",
    "result,pred=mcmc.sampler()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
